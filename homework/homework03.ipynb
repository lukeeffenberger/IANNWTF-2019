{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03 - Backpropagation\n",
    "\n",
    "In this homework you will implement the backpropagation algorithm in NumPy.  \n",
    "You will use it to train a simple feed forward neural network to solve the XOR problem.\n",
    "\n",
    "There are six subtasks in this homework:\n",
    "* Differentiate the activation function.\n",
    "* Explain the backpropagation rule for the biases.\n",
    "* Update the implementation of the perceptron.\n",
    "* Implement the multi-layer perceptron.\n",
    "* Train the multi-layer perceptron.\n",
    "* Visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "ts = np.array([0,1,1,0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the following model\n",
    "\n",
    "<img src=\"mlp_xor.png\" width=\"500\">\n",
    "\n",
    "The activation function for all neurons is the logistic function\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting backprop right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the activation function.\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the backpropagation algorithm. For that let's have a look at the formula:\n",
    "\n",
    "$$\\begin{aligned} \\frac{\\partial L}{\\partial w_{ij}^{(l)}} &= \\delta_i^{(l)} \\ {a_j}^{(l-1)} \\end{aligned}$$\n",
    "\n",
    "with \n",
    "\n",
    "$$ \\delta_i^{(l)} = \\begin{cases} - (t_i - y_i) \\ \\sigma'({d_i}^{(N)}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  if \\  l=N, \\\\ \\left( \\sum_{k=1}^{m}\\delta_k^{(l+1)}  w_{ki}^{(+1)}\\right ) \\sigma'({d_i}^{(l)}) \\ \\ \\   else. \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see we need the first derivative of the activation function.\n",
    "# Differentiate and implement it. You don't need to provide your differentiation in this notebook!\n",
    "def sigmoidprime(x):\n",
    "    return ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we only derived the backpropagation formula for the weights. Now that we implement it we also need the formula for the biases:\n",
    "\n",
    "$$\\begin{aligned} \\frac{\\partial L}{\\partial b_{i}^{(l)}} &= \\delta_i^{(l)}  \\end{aligned}$$\n",
    "\n",
    "with \n",
    "\n",
    "$$ \\delta_i^{(l)} = \\begin{cases} - (t_i - y_i) \\ \\sigma'({d_i}^{(N)}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  if \\  l=N, \\\\ \\left( \\sum_{k=1}^{m}\\delta_k^{(l+1)}  w_{ki}^{(+1)}\\right ) \\sigma'({d_i}^{(l)}) \\ \\ \\   else. \\end{cases} $$\n",
    "\n",
    "Please explain how this formula comes about! You do not have to provide any formal derivation. 1 or 2 sentences should be enough!\n",
    "\n",
    "*Answer*: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to update our perceptron from last week a little bit. So you can reuse a few parts from last time!\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units):\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Initialize random weights and a random bias term. \n",
    "        # The weights with mean 0 and stddev 0.5.\n",
    "        # The bias with mean 0 and stddev 0.05. Check 'np.random.normal()'.\n",
    "        \n",
    "        # Define the learning rate as 1.\n",
    "        \n",
    "        #######################\n",
    "        \n",
    "        # Further we will later need access to the input and drive of the neuron. \n",
    "        # We initialize variables to store it.\n",
    "        self.inputs = 0\n",
    "        self.drive = 0\n",
    "        \n",
    "        \n",
    "    def forward_step(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Calculate the drive and store it in the corresponding variable.\n",
    "        \n",
    "        # Return the activation.\n",
    "        return \n",
    "        ######################\n",
    "        \n",
    "        \n",
    "    def update(self, delta):\n",
    "        # We will call this function to update the parameters for this specific perceptron.\n",
    "        # The function is provide with a delta. So you only need to compute the gradients \n",
    "        # perform the update.\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the gradient.\n",
    "        \n",
    "        # Update weights and bias.\n",
    "        \n",
    "        ####################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now build a multi-layer perceptron out of the previously defined perceptrons.\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Here we initialize the perceptrons for the hidden layer.\n",
    "        self.hidden_layer = [\n",
    "            Perceptron(input_units=2),\n",
    "            Perceptron(input_units=2),\n",
    "            Perceptron(input_units=2),\n",
    "            Perceptron(input_units=2)\n",
    "        ]\n",
    "        # Initializing the output neuron.\n",
    "        self.output_neuron = Perceptron(input_units=4)\n",
    "        # Initializing a variable to store the output.\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward_step(self, inputs):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the activations for the hidden layer.\n",
    "        \n",
    "        # You might need to reshape ((4,1)->(4,)) the resulting array to feed it to the output neuron. \n",
    "        # Check 'np.reshape(arr, newshape=(-1)).'\n",
    "        \n",
    "        # Compute the activation of the output neuron and store it in 'self.output'.\n",
    "        \n",
    "        ######################\n",
    "        \n",
    "    def backprop_step(self, inputs, target):\n",
    "        # Use the Sum-squared error (lecture 3) as the loss function.\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the delta at the output neuron.\n",
    "        \n",
    "        # Update the parameters of the output neuron.\n",
    "        \n",
    "        # Compute the deltas for the hidden neurons.\n",
    "        \n",
    "        # Update the parameters for all four neurons in the hidden layer.\n",
    "        \n",
    "        ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP.\n",
    "mlp = MLP()\n",
    "# Initialize lists to store epochs, loss, accuracy.\n",
    "epochs=[]\n",
    "losses=[]\n",
    "accuracies=[]\n",
    "\n",
    "for epoch in range(500):\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    accuracy_buffer = 0\n",
    "    loss_buffer = 0\n",
    "    \n",
    "    # Training loop.\n",
    "    for i in range(4):\n",
    "        x = xs[i]\n",
    "        t = ts[i]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Perform a forward step with the given sample.\n",
    "\n",
    "        # Perform a backpropagation step with the given sample and target.\n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        accuracy_buffer += int(float(mlp.output>=0.5) == t)\n",
    "        loss_buffer += (t-mlp.output)**2\n",
    "        \n",
    "    accuracies.append(accuracy_buffer/4.0)\n",
    "    losses.append(loss_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training progress. Loss and accuracy.\n",
    " \n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
