{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpURKI2bLfRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import gym\n",
        "%matplotlib notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzrsdHSrMZm6",
        "colab_type": "text"
      },
      "source": [
        "In this weeks homework we expect you to implement a DQN. With the final project getting closer we expect you to implement this task on your own with little guidance. The slides contain Pseudocode for the classic DQN implementation which makes use of experience replay and the Watkins Q-Learning TD-error. Feel free to go for experience replay or just use the samples from your most recent batch of iterations without experience replay, both should work fine. Also instead of the Watkins TD-error you might opt for any of the other introduced TD-errors.\n",
        "\n",
        "Remarks: \n",
        " - we use ([openai gym](https://gym.openai.com/)) to get access to the cartpole-v1 environment\n",
        " - following the [documentation](https://gym.openai.com/docs/) you need gym.make(...), gym.reset() and gym.step(...)\n",
        " - your network should be **very** small - try out one or two hidden layers of no more than 16 units\n",
        " - check [cartpole-v1](https://gym.openai.com/envs/CartPole-v1/) out and try to get an intuition on what you are trying to achieve\n",
        " - you dont have to hassle with the action_space argument, the agent has exactly two possible actions: go left and go right - what does that imply for the output of your network (how many Q-value estimations do you need?)\n",
        " - If the lecture was not enough and you need some ideas on how to structure your code, check this [blogpost](https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288) out. Don't just copy their code though, we expect you to write your own code, your own network, etc. \n",
        " - Visualizing the training with gym.render might be interesting. It is quite a hassle with colab though, so either check [this solution](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/) out or just run and render it locally on your machine\n",
        " - It is totally ok if you need more help. Feel free to join the Telegram QnA group ([invite link](https://t.me/joinchat/BO4SABd7bdhmSHOcP1rrow)), drop by at the QnA on friday, additionally there will be a QnA on monday, 12-14 again (room still to be announced). Worst case just write a mail to lschmid@uos.de\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG5aZb6BLiRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Your Code"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}