{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10 - Word Embeddings\n",
    "\n",
    "In this homework you will train the skip gram model on the bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are provided to generate two dictionaries to translate words into ids and back.\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text_lower = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_tokenized = tokenizer.tokenize(text_lower)\n",
    "    return text_tokenized\n",
    "\n",
    "def create_dicts_from_tokenized_text(tokenized_text,vocabulary_size):\n",
    "    words_and_count = Counter(tokenized_text).most_common(vocabulary_size - 1)\n",
    "    print(words_and_count)\n",
    "    word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
    "    word2id[\"_UNKNOWN_\"] = 0\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and tokenize the bible text.\n",
    "bible_text = open('bible.txt').read()\n",
    "bible_text_tokenized = tokenize_text(bible_text)\n",
    "\n",
    "# Create dictionaries for the 10000 most common words.\n",
    "vocabulary_size = 10000\n",
    "word2id, id2word = create_dicts_from_tokenized_text(bible_text_tokenized, vocabulary_size)\n",
    "\n",
    "# Translate the tokenized text into ids.\n",
    "bible_id = [word2id.get(word, 0) for word in bible_text_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset of all pairs of word and context with a context window of 2.\n",
    "# Tip: Create two lists: \n",
    "#          - one long list with all valid word indices (for which you can apply the context windows)\n",
    "#          - one short list with the shifts for getting the context word (remember to exclude 0)\n",
    "#      Then generate two lists (input words + context words), by running through the two lists above.\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# Define the context range.\n",
    "\n",
    "# Only use the words that have enough words before and after it for getting the contexts.\n",
    "\n",
    "# Generate one list of the words and on for the contexts. If you have e.g. 4 context words for each word\n",
    "# remember that you need each word 4 times in the list for the words.\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from these lists. Batch size: 128. Shuffle.\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "                                shape=(self.vocab_size, self.embedding_size),\n",
    "                                initializer='GlorotNormal'\n",
    "                                )\n",
    "        self.score_matrix = self.add_weight(\n",
    "                            shape=(self.vocab_size, self.embedding_size),\n",
    "                            initializer='GlorotNormal'\n",
    "                            )\n",
    "        self.score_bias = self.add_weight(\n",
    "                            shape=(self.vocab_size),\n",
    "                            initializer='zeros'\n",
    "                            )\n",
    "        \n",
    "    def call(self,inputs,labels):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to readout the nearest neighbors inside the embedding.\n",
    "# Feel free to add more words to the list.\n",
    "target_words = ['israel', 'god', 'jesus', '1', 'love', 'day', 'wine']\n",
    "number_of_nearest_neighbors = 8\n",
    "\n",
    "def find_and_print_nearest_neighbors(target_words, number_of_nearest_neighbors,embeddings):\n",
    "    normed_embeddings = embeddings / np.sqrt(np.sum(embeddings**2, axis=1, keepdims=True))\n",
    "    for word in target_words:\n",
    "        word_id = word2id[word]\n",
    "        word_embedding = normed_embeddings[word_id, :]\n",
    "        cosine_similarities = np.matmul(normed_embeddings, word_embedding )\n",
    "        n_nearest_neighbors = np.argsort(-cosine_similarities)[:number_of_nearest_neighbors]\n",
    "        print(\"Nearest to \" + word + \": \" + \", \".join([id2word[nearest] for nearest in n_nearest_neighbors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "#####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
