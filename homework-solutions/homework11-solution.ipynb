{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11 - Generative Models\n",
    "\n",
    "In this homework you will implement a simple generative adversarial network to generate new samples for the MNIST dataset. \n",
    "\n",
    "On the technical side you will learn about TensorBoard and how to use it to store your metrics and also the stored images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset. Training data is enough. Normalize to [-1,1]. Add channel dimension of depth 1.\n",
    "# Format to float32.\n",
    "### YOUR CODE HERE ###\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = (train_images/255.0)*2 - 1\n",
    "train_images = np.reshape(train_images, newshape=[-1,28,28,1])\n",
    "train_images = train_images.astype(np.float32)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator:\n",
    "# Dense layer: 7*7*64 neurons + BatchNorm + LeakyReLU\n",
    "# Reshape to (batch size, 7, 7, 64)\n",
    "# Transpose Convolutional Layer: 32 kernels of size (5,5) with strides (1,1) + \"same\" padding + BatchNorm + LeakyReLU\n",
    "# Transpose Convolutional Layer: 16 kernels of size (5,5) with strides (2,2) + \"same\" padding + BatchNorm + LeakyReLU\n",
    "# Transpose Convolutional Layer: 1 kernels of size (5,5) with strides (2,2) + \"same\" padding + TanH\n",
    "# All layers in the generator don't use biases! Set parameter use_bias=False.\n",
    "# NOTE: Go back to lecture 7 to make sure you use BatchNorm in combination with an \n",
    "# activation function in the correct way.\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "class Generator(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "                            units=7*7*64,\n",
    "                            activation=None,\n",
    "                            use_bias=False\n",
    "            \n",
    "        )\n",
    "        self.batchnorm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.convT_1 = tf.keras.layers.Conv2DTranspose(\n",
    "                            filters=32,\n",
    "                            kernel_size=5,\n",
    "                            strides=1,\n",
    "                            padding='SAME',\n",
    "                            activation=None,\n",
    "                            use_bias=False\n",
    "        )\n",
    "        self.batchnorm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.convT_2 = tf.keras.layers.Conv2DTranspose(\n",
    "                            filters=16,\n",
    "                            kernel_size=5,\n",
    "                            strides=2,\n",
    "                            padding='SAME',\n",
    "                            activation=None,\n",
    "                            use_bias=False\n",
    "        )\n",
    "        self.batchnorm_3 = tf.keras.layers.BatchNormalization()\n",
    "        self.convT_3 = tf.keras.layers.Conv2DTranspose(\n",
    "                            filters=1,\n",
    "                            kernel_size=5,\n",
    "                            strides=2,\n",
    "                            padding='SAME',\n",
    "                            activation=tf.nn.tanh,\n",
    "                            use_bias=False\n",
    "        )\n",
    "        \n",
    "    def call(self,x,is_training):\n",
    "        x = self.dense(x)\n",
    "        x = self.batchnorm_1(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = tf.reshape(x, shape=(-1, 7, 7, 64))\n",
    "        x = self.convT_1(x)\n",
    "        x = self.batchnorm_2(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.convT_2(x)\n",
    "        x = self.batchnorm_3(x, training=is_training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.convT_3(x)\n",
    "        return x\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator:\n",
    "# Conv layer: 8 kernels of size (5,5) with strides (2,2) + \"same\" padding + LeakyReLU\n",
    "# Conv layer: 16 kernels of size (5,5) with strides (2,2) + \"same\" padding + LeakyReLU\n",
    "# Flatten\n",
    "# Dense layer: 1 unit, no activation + Sigmoid activation\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "class Discriminator(tf.keras.layers.Layer):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv_1 = tf.keras.layers.Conv2D(\n",
    "                            filters = 8,\n",
    "                            kernel_size =5, \n",
    "                            strides=2,\n",
    "                            padding=\"SAME\",\n",
    "                            activation=tf.nn.leaky_relu\n",
    "                            \n",
    "        )\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(\n",
    "                            filters = 16,\n",
    "                            kernel_size =5, \n",
    "                            strides=2,\n",
    "                            padding=\"SAME\",\n",
    "                            activation=tf.nn.leaky_relu\n",
    "        )        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "                            units = 1,\n",
    "                            activation=tf.nn.sigmoid\n",
    "        )\n",
    "        \n",
    "    def call(self, x, is_training):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset. Shuffle.\n",
    "# We will train with batches of 64 (32 real images, 32 fake images).\n",
    "# So batch in 32.\n",
    "### YOUR CODE HERE ###\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(batch_size=32)\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the losses. It makes sense to first program the training loop and then come back here!\n",
    "\n",
    "# Define the loss for the generator.\n",
    "def generator_loss(probabilities):\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Get only the output probabilities for the fake images.\n",
    "    probabilities_fake = probabilities[:32]\n",
    "    # Create the label vector indicating that the images are correct (=1).\n",
    "    labels_one = tf.convert_to_tensor(np.ones(shape=(32,1)))\n",
    "    # Use binary cross entropy loss to compute the loss. tf.keras.losses.BinaryCrossentropy()\n",
    "    binary = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = binary(labels_one,probabilities_fake)\n",
    "    ########################\n",
    "    return loss\n",
    "\n",
    "# Define the loss for the discriminator.\n",
    "def discriminator_loss(probabilities):\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Create the label vector indicating which images are real and which are fake.\n",
    "    labels_one = tf.convert_to_tensor(np.ones(shape=(32,1)))\n",
    "    labels_zero = tf.convert_to_tensor(np.zeros(shape=(32,1)))\n",
    "    labels = tf.concat([labels_zero, labels_one], axis=0)\n",
    "    # Use binary cross entropy loss to compute the loss.\n",
    "    binary = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = binary(labels,probabilities)\n",
    "    ########################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ \n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# For me the in-notebook tensorboard sometimes doesn't work. In this case maybe just use your terminal\n",
    "# if you work on your own machine. (comment out the following line)\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialzing generator and discriminator.\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "# Define size of latent variable vector.\n",
    "z_dim = 50\n",
    "# Define random seed to use for generating 8 images for supervision.\n",
    "seed = tf.random.normal(shape=[8,z_dim])\n",
    "\n",
    "# Initialize two optimizers (one for generator, one for discriminator).\n",
    "# During training you will have to do every step twice (computing loss, computing gradients,\n",
    "# applying gradients, storing loss in summary). Namely once for generator and once for discriminator.\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "dis_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "step=0\n",
    "\n",
    "for epochs in range(6):\n",
    "    for real in dataset:\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            \n",
    "            ### YOUR CODE HERE ###\n",
    "            # Generate random noise vector. tf.random.normal()\n",
    "            noise = tf.random.normal(shape=[32, z_dim])\n",
    "            # Generate fake images with generator.\n",
    "            fake = generator(noise, is_training=True)\n",
    "            # Merge fake and real images to a long vector. tf.concat()\n",
    "            images = tf.concat([fake, real], axis=0)\n",
    "            # Compute output from discriminator.\n",
    "            probs = discriminator(images, is_training=True)\n",
    "            \n",
    "            # Compute loss, compute gradients, apply gradients, store summaries.\n",
    "            gen_loss = generator_loss(probs)\n",
    "            dis_loss = discriminator_loss(probs)\n",
    "            gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "            dis_gradients = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "\n",
    "            gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "            dis_optimizer.apply_gradients(zip(dis_gradients, discriminator.trainable_variables))\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('generator_loss', gen_loss, step=step)\n",
    "                tf.summary.scalar('discriminator_loss', dis_loss, step=step)\n",
    "            ########################\n",
    "            \n",
    "        # Every 100 steps generate images from the defined seed and \n",
    "        # store to supervise how well the generator works.\n",
    "        if step % 100 == 0:\n",
    "            ### YOUR CODE HERE ###\n",
    "            fake = generator(seed, is_training=False)\n",
    "            with test_summary_writer.as_default():\n",
    "                tf.summary.image('fake_images', fake, step=step, max_outputs=8)\n",
    "            ########################\n",
    "            \n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
