{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 09 - RNNs\n",
    "\n",
    "In this homework you will learn how to generate text with an RNN. \n",
    "\n",
    "Defining an RNN in TensorFlow is based on a specific framework. Therefore I will provide you with the correct model definition. Your task will be to to understand how the model processes sequential data, which kind of data it returns and how to train it.\n",
    "\n",
    "You will train a stacked RNN to generate text passages, similar to those in the bible.\n",
    "For that make sure that the 'bible.txt' (from Stud.IP) file is in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text.\n",
    "txt = open(\"bible.txt\",'r').read()\n",
    "print(\"Text length: {}\".format(len(txt)))\n",
    "print('-------------------------------------')\n",
    "\n",
    "# Inspect first lines:\n",
    "print(txt[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary of the text\n",
    "vocab = list(set(txt))\n",
    "print(\"Vocabulary: {}\".format(vocab))\n",
    "print('--------------------------')\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to switch between the indices of the characters and the characters themselves.\n",
    "char2idx = {ch:i for i,ch in enumerate(vocab)}\n",
    "idx2char = {i:ch for i,ch in enumerate(vocab)}\n",
    "\n",
    "# Translate the text to indices.\n",
    "txt_idx = [char2idx[ch] for ch in txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare TensorFlow dataset.\n",
    "\n",
    "We will train our RNNs with TBPTT(k1=20, k2=20).\n",
    "In the following we will process the text to a suitable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a tensorflow dataset out of the text (in indices). (tf.data.Dataset.from_tensor_slices)\n",
    "### YOUR CODE HERE ###\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(txt_idx)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will trainon subsequences of length 20 and compute the loss for each timestep.\n",
    "# Let's think about how a single training datapoint will look.\n",
    "# Example:\n",
    "# Input sequence: \"Moses:  Called Genesi\"\n",
    "# Target sequence: \"oses:  Called Genesis\"\n",
    "# To create these pairs of sequence we chunk the dataset into subsequences of length k+1.\n",
    "# You can use .batch() for this. \n",
    "# And make sure that all subsequences in the resulting dataset have length k+1 (understand\n",
    "# parameter 'drop_remainder' in .batch())\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "k = 20\n",
    "sequences = text_dataset.batch(k+1, drop_remainder=True)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to map each sequence of length 21\n",
    "# to a (input, target) pair.\n",
    "# Given the following function you can use the dataset method .map() here.\n",
    "def input_target_split(seq):\n",
    "    return seq[:-1], seq[1:]\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "dataset = sequences.map(input_target_split)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as usual we shuffle our dataset and chunk it into batches of 64.\n",
    "### YOUR CODE HERE ###\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(64, drop_remainder=True)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided definitions of Vanilla RNN cell and RNN model.\n",
    "\n",
    "class VanillaRNNCell(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim, units):\n",
    "        super(VanillaRNNCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.units = units\n",
    "        # TF needs this.\n",
    "        self.state_size = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w_in = self.add_weight(\n",
    "                            shape=(self.input_dim, self.units),\n",
    "                            initializer='uniform'\n",
    "                            )\n",
    "        self.w_h = self.add_weight(\n",
    "                            shape=(self.units, self.units),\n",
    "                            initializer='uniform'\n",
    "                            )\n",
    "        self.b_h = self.add_weight(\n",
    "                            shape=(self.units,),\n",
    "                            initializer='zeros'\n",
    "                            )       \n",
    "            \n",
    "    def call(self, inputs, hidden_states):\n",
    "        h_prev = hidden_states[0]\n",
    "        h_new = tf.nn.sigmoid(tf.matmul(inputs, self.w_in) + tf.matmul(h_prev, self.w_h) + self.b_h)\n",
    "        return h_new, [h_new]\n",
    "\n",
    "state_size_1 = 128\n",
    "state_size_2 = 256\n",
    "\n",
    "class RNN(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.cell_1 = VanillaRNNCell(input_dim=vocab_size, units=state_size_1)\n",
    "        self.cell_2 = VanillaRNNCell(input_dim=state_size_1, units=state_size_2)\n",
    "        self.cells = [self.cell_1, self.cell_2]\n",
    "        self.rnn = tf.keras.layers.RNN(self.cells, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self,x):\n",
    "        seqs = self.rnn(x)\n",
    "        output = self.output_layer(seqs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT MIGHT MAKE SENSE TO FIRST CODE THE ACTUAL TRAINING CELL (NEXT ONE) AND THEN COME BACK HERE.\n",
    "\n",
    "# Defining the function for generating novel text samples.\n",
    "# The function should take the sample below (of length k) and generate a text sequence of length k+n from it.\n",
    "\n",
    "sample = 'And God and Jesus go'\n",
    "assert(len(sample)==k)\n",
    "\n",
    "def generate_sample(sample, n):\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    # Translate sample string into list of characters.\n",
    "    sample_idx = []\n",
    "    for ch in sample:\n",
    "        sample_idx.append(char2idx[ch])\n",
    "    \n",
    "    # Transform list into tensor of shape (1,20,vocab_size)\n",
    "    sample_idx = np.array(sample_idx)\n",
    "    sample_idx = np.reshape(sample_idx, newshape=(1,k))\n",
    "    sample_idx = tf.convert_to_tensor(sample_idx)\n",
    "    sample_idx_onehot = tf.one_hot(sample_idx, depth=vocab_size)\n",
    "    ######################\n",
    "    \n",
    "    # Sample n new characters.\n",
    "    for _ in range(n):\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Feed sample sequence into RNN and get probabilities of next character.\n",
    "        softmax_out = rnn(sample_idx_onehot).numpy()[:,-1]\n",
    "        logits = tf.math.log(softmax_out) \n",
    "        # Sample index for new character (use tf.random.categorical()).\n",
    "        sampled_idx = tf.random.categorical(logits, 1).numpy()[0,0]\n",
    "        # Translate to actual character and add it to sample string.\n",
    "        sampled_char = idx2char[sampled_idx]\n",
    "        sample += sampled_char\n",
    "\n",
    "        # Create new sequence of 20 indices by deleting the first character of the old sequence\n",
    "        # and adding the new character.\n",
    "        new_idx_onehot = tf.one_hot(sampled_idx, depth=vocab_size)\n",
    "        new_idx_onehot = tf.reshape(new_idx_onehot, shape=[1,1,vocab_size])\n",
    "        sample_idx_onehot = tf.concat([sample_idx_onehot[:,1:,:], new_idx_onehot], axis=1)   \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "# Initialize the RNN, cross entropy as a loss function and as an optimizer Adam with learning rate 0.01.\n",
    "rnn = RNN()\n",
    "ce = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "step = 0\n",
    "\n",
    "\n",
    "# Train for one epoch. Your loss should be around 1.4.\n",
    "# Remember to encode the inputs and target values as one hots.\n",
    "for _ in range(1):\n",
    "    for x, t in dataset:\n",
    "        x = tf.one_hot(x, depth = 74)\n",
    "        t = tf.one_hot(t, depth = 74)\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = rnn(x)\n",
    "            loss = ce(t, out)\n",
    "            gradients = tape.gradient(loss, rnn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, rnn.trainable_variables))          \n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        # After each 20 training steps generate a sample using the function defined above. \n",
    "        # And print loss to supervise the model.\n",
    "        if step % 20 == 0:\n",
    "            print(\"Loss: {}\".format(loss))\n",
    "            print(generate_sample(sample,100))\n",
    "            print('--------------------------')\n",
    "######################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to generate some funny samples.\n",
    "print(generate_sample(sample,1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
