{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05 - More on DNNs\n",
    "\n",
    "In this homework you will deepen your understanding of TensorFlow. You will implement a simple classifier for the MNIST dataset.    \n",
    " \n",
    "Be aware that sometimes there are variable names in the provided code that need to align with variables that you define. You can either change my or your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow has some popular datasets already included. \n",
    "# Check out https://www.tensorflow.org/api_docs/python/tf/keras/datasets.\n",
    "# In this homework we will work with the popular MNIST dataset.\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is always important to understand the format and the count of the data you are dealing with.**  \n",
    "**Please answer the following questions:**\n",
    "\n",
    "Q: *How many training/test images does this dataset have?*  \n",
    "A: 60000 training images, 10000 test images\n",
    "\n",
    "Q: *Which shape do the images have?*  \n",
    "A: 28 times 28 pixels\n",
    "\n",
    "Q: *In which range are the pixel values?*  \n",
    "A: grey-scale from 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should make use of the following functions and properties.\n",
    "# You can access the shape of an array 'arr' with 'arr.shape'.\n",
    "# You can use 'np.max', 'np.min' to access the maximum or minimum of an array.\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(np.max(train_images[0]))\n",
    "print(np.min(train_images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can also be helpful to have a proper look at specific samples of a dataset to understand what you are dealing with.\n",
    "# Plot 5 samples with their corresponding label.\n",
    "\n",
    "fig, ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Readout an image and the corresponding label.\n",
    "    img = train_images[i]\n",
    "    lbl = train_labels[i]\n",
    "    ######################\n",
    "    ax[i].imshow(img, cmap='gray')\n",
    "    ax[i].set_title(lbl)\n",
    "    ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "# It makes sense to shuffle the training dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=60000)\n",
    "# Further we have seen that it is advantageous to train with mini-batches instead of one sample at a time.\n",
    "train_dataset = train_dataset.batch(128)\n",
    "# Let's have a look at how the elements of the dataset now look.\n",
    "for elem in train_dataset:\n",
    "    print(elem[0].shape)\n",
    "    print(elem[1].shape)\n",
    "    break\n",
    "# The test dataset can be processed in full batch.\n",
    "test_dataset = test_dataset.batch(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement a simple fully connected feed forward neural network like the last time.\n",
    "#\n",
    "# Our network will have the following layers:\n",
    "# Input : 784 units.\n",
    "# Hidden layer 1: 256 units. With sigmoid activation function.\n",
    "# Hidden layer 2: 256 units. With sigmoid activation function.\n",
    "# Output: 10 units. With softmax activation function.\n",
    "# \n",
    "# Instead of implementing our own layer we can directly implement the network using pre-built layers \n",
    "# from TensorFlow.\n",
    "# For that check out 'tf.keras.layers.Dense(units= , activation=)'. It is basically the same layer that we\n",
    "# implemented by hand last time.\n",
    "# For activations functions check out 'tf.keras.activations'.\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Model(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Define the three layers.\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=256,\n",
    "                                               activation=tf.keras.activations.sigmoid\n",
    "                                               )\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=256,\n",
    "                                               activation=tf.keras.activations.sigmoid\n",
    "                                               )\n",
    "        self.output_layer = tf.keras.layers.Dense(units=10,\n",
    "                                               activation=tf.keras.activations.softmax\n",
    "                                               )\n",
    "        ######################\n",
    "        \n",
    "    def call(self, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Define the forward step.\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        ######################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "*Q: How many parameters does this network have?*  \n",
    "A: 784x256 + 256 + 256x256 + 256 + 256x10 + 10 = 269322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# Initialize the model.\n",
    "model = Model()\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Initialize the optimizer: Adam with default parameters. Check out 'tf.keras.optimizers'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "######################\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_steps = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_steps = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "step = 0\n",
    "\n",
    "\n",
    "# We train for 3 epochs.\n",
    "for epoch in range(3):\n",
    "    \n",
    "    for (x,t) in train_dataset:\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # We need to reshape the input from [batchsize,28,28] to [batchsize,784].\n",
    "        x = tf.reshape(x, shape=(-1,784))\n",
    "        # Further we need to transform the labels into a one-hot encoding.\n",
    "        # Check out 'tf.one_hot'.\n",
    "        t = tf.one_hot(t, depth=10)\n",
    "        ######################\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the output, loss and the gradients.\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(x)\n",
    "            loss = cross_entropy_loss(t, output)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # Apply gradients.\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "        ######################\n",
    "        \n",
    "        # Compute the accuracy.\n",
    "        accuracy = np.sum(np.argmax(t, axis=1) == np.argmax(output, axis=1)) / t.shape[0]\n",
    "        # Store loss and accuracy.\n",
    "        train_accuracies.append(accuracy)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        # After each 25 training step we check the generalization to the test dataset.\n",
    "        if step % 25 == 0:\n",
    "            for (x,t) in test_dataset:\n",
    "                ### YOUR CODE HERE ###\n",
    "                x = tf.reshape(x, shape=(-1,784))\n",
    "                t = tf.one_hot(t, depth=10)\n",
    "                output = model(x)\n",
    "                loss = cross_entropy_loss(t, output)\n",
    "                accuracy = np.sum(np.argmax(t, axis=1) == np.argmax(output, axis=1)) / t.shape[0]\n",
    "                ######################\n",
    "                test_steps.append(step)\n",
    "                test_accuracies.append(accuracy)\n",
    "                test_losses.append(loss)\n",
    "        \n",
    "        train_steps.append(step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize accuracy and loss for training and test data. \n",
    "# One plot training and test loss.\n",
    "# One plot training and test accuracy.\n",
    "### YOUR CODE HERE###\n",
    "plt.figure()\n",
    "line1, = plt.plot(train_steps, train_losses)\n",
    "line2, = plt.plot(test_steps, test_losses)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend((line1,line2),(\"training\",\"test\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "line1, = plt.plot(train_steps, train_accuracies)\n",
    "line2, = plt.plot(test_steps, test_accuracies)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend((line1,line2),(\"training\",\"test\"))\n",
    "plt.show()\n",
    "#####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
